{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bob Sadino bilang orang goblog cenderung lebih...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sekolahan itu kek mantan makin oke semenjak di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bagus jg buat naikin standard amplop jelang 17...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ada apa dg demokrasi Indonesia !</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bapak Prabowo ibu Jokowi . Santai .. :) #debat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17713</th>\n",
       "      <td>AKU BANGGA KARNA GUBERNUR KU BELIAUUUU. tetap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17714</th>\n",
       "      <td>Terima Kasih Pak @aniesbaswedan #4niesTenggela...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17715</th>\n",
       "      <td>Keren ne, Gub Formula E-mpang Wajah tersenyum ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17716</th>\n",
       "      <td>Banjir jakarta bukan salah @aniesbaswedan tapi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17717</th>\n",
       "      <td>Presiden Jokowi Minta 4nies Keruk Waduk untuk ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17718 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  label\n",
       "0      Bob Sadino bilang orang goblog cenderung lebih...      0\n",
       "1      Sekolahan itu kek mantan makin oke semenjak di...      0\n",
       "2      Bagus jg buat naikin standard amplop jelang 17...      0\n",
       "3                       Ada apa dg demokrasi Indonesia !      0\n",
       "4      Bapak Prabowo ibu Jokowi . Santai .. :) #debat...      0\n",
       "...                                                  ...    ...\n",
       "17713  AKU BANGGA KARNA GUBERNUR KU BELIAUUUU. tetap ...      1\n",
       "17714  Terima Kasih Pak @aniesbaswedan #4niesTenggela...      1\n",
       "17715  Keren ne, Gub Formula E-mpang Wajah tersenyum ...      1\n",
       "17716  Banjir jakarta bukan salah @aniesbaswedan tapi...      1\n",
       "17717  Presiden Jokowi Minta 4nies Keruk Waduk untuk ...      1\n",
       "\n",
       "[17718 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/twitter_indonesia_sarcastic/raw_data/khotijah.csv\")\n",
    "df.columns = [\"tweet\", \"label\"]\n",
    "# un-reverse\n",
    "df[\"tweet\"] = df[\"tweet\"].apply(lambda x: \" \".join(x.split()[::-1]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 13368, 1: 4350})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lsh import minhash, cache\n",
    "\n",
    "# use minHash LSH algorithm to find near duplicates\n",
    "hasher = minhash.MinHasher(seeds=100, char_ngram=4, hashbytes=8, random_state=42)\n",
    "lsh_cache = cache.Cache(num_bands=20, hasher=hasher)\n",
    "neardup_ids = []\n",
    "\n",
    "# hash every tweet\n",
    "for idx, text in enumerate(df[\"tweet\"]):\n",
    "    lsh_cache.add_fingerprint(hasher.fingerprint(text), idx)\n",
    "\n",
    "# find bins of duplicates\n",
    "for cache_bin in lsh_cache.bins:\n",
    "    for bucket_id in cache_bin:\n",
    "        if len(cache_bin[bucket_id]) > 1:\n",
    "            # add ids of neardup texts\n",
    "            neardup_ids.append(cache_bin[bucket_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sets = sorted(neardup_ids, key=lambda x: min(x))\n",
    "\n",
    "merged_sets = []\n",
    "current_merged_set = sorted_sets[0]\n",
    "\n",
    "for s in sorted_sets[1:]:\n",
    "    # if has overlapping element\n",
    "    if any(x in current_merged_set for x in s):\n",
    "        current_merged_set.update(s)  # merge overlapping sets\n",
    "    else:\n",
    "        merged_sets.append(current_merged_set)\n",
    "        current_merged_set = s  # start a new merged set\n",
    "\n",
    "# add last set\n",
    "merged_sets.append(current_merged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ids = set()\n",
    "\n",
    "# for each \"cluster\", only keep first and drop the rest\n",
    "for cluster in merged_sets:\n",
    "    drop_ids |= set(list(cluster)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda row: row.name not in drop_ids, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 12190, 1: 671})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mask_tweets(tweet):\n",
    "    # Mask usernames with <username>\n",
    "    tweet = re.sub(r'@[\\w]+', '<username>', tweet)\n",
    "    # Mask hashtags with <hashtag>\n",
    "    tweet = re.sub(r'#[\\w]+', '<hashtag>', tweet)\n",
    "    # Mask email addresses with <email>\n",
    "    tweet = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w{2,4}\\b', '<email>', tweet)\n",
    "    # Mask links/URLs with <link> (handling various URL formats)\n",
    "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '<link>', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1s/0qwf_tdx2yn3p49d6fcfgt500000gp/T/ipykernel_6576/1901210317.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"tweet\"] = df[\"tweet\"].apply(mask_tweets)\n"
     ]
    }
   ],
   "source": [
    "df[\"tweet\"] = df[\"tweet\"].apply(mask_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/twitter_indonesia_sarcastic/raw_data/khotijah_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcastic = df[df['label'] == 1]\n",
    "non_sarcastic = df[df['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_non_sarcastic = non_sarcastic.sample(n=len(sarcastic) * 3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = pd.concat([sampled_non_sarcastic, sarcastic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_val_df = train_test_split(balanced_df, train_size=0.7, random_state=41, stratify=balanced_df['label'])\n",
    "val_df, test_df = train_test_split(test_val_df, test_size=(2/3), random_state=41, stratify=test_val_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1878, 538, 268)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df), len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"../data/twitter_indonesia_sarcastic/data/train.csv\", index=False)\n",
    "test_df.to_csv(\"./data./twitter_indonesia_sarcastic/data/test.csv\", index=False)\n",
    "val_df.to_csv(\"./data./twitter_indonesia_sarcastic/data/validation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
