{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/root/reddit_comments_subreddit_indonesia_RC_2023-01-09.json\") as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "929182"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"author\", \"created_utc\", \"score\", \"permalink\", \"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data = []\n",
    "\n",
    "for datum in raw_data:\n",
    "    # split multi-sentence\n",
    "    body = datum[\"body\"]\n",
    "    sentences = body.split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        obj = {k: datum[k] for k in columns}\n",
    "        text = sentence.strip()\n",
    "        if len(text) > 0:\n",
    "            obj['body'] = text\n",
    "            sentences_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1416946/1416946 [00:25<00:00, 55103.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from ftlangdetect import detect\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "valid_langs = [\"id\", \"jv\", \"min\", \"ms\", \"su\"]\n",
    "indonesia_data = []\n",
    "\n",
    "for datum in tqdm(sentences_data):\n",
    "    lang = detect(datum[\"body\"])['lang']\n",
    "    if lang in valid_langs:\n",
    "        datum['lang_fastText'] = lang\n",
    "        indonesia_data.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784942"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indonesia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_sarcasm_tag(text: str) -> bool:\n",
    "    # ends with either one of these sarcasm tags\n",
    "    pattern = r'(?<!\\S)(?:/s|//s|/sarcasm|//sarcasm|\\\\s|\\\\\\\\s|\\\\sarcasm|\\\\\\\\sarcasm)$'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "\n",
    "for datum in indonesia_data:\n",
    "    body = datum[\"body\"]\n",
    "    tags = has_sarcasm_tag(body)\n",
    "    if tags:\n",
    "        # make `text` column clean; remove sarcasm tags\n",
    "        for tag in tags:\n",
    "            body = body.replace(tag, \"\")\n",
    "\n",
    "    datum[\"label\"] = 1 if tags else 0\n",
    "    datum[\"text\"] = body.strip()\n",
    "    cleaned_data.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(cleaned_data)\n",
    "df['created_utc'] = df['created_utc'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 783944, 1: 998})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 784942/784942 [01:56<00:00, 6746.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from lsh import minhash, cache\n",
    "\n",
    "# use minHash LSH algorithm to find near duplicates\n",
    "hasher = minhash.MinHasher(seeds=100, char_ngram=4, hashbytes=8, random_state=42)\n",
    "lsh_cache = cache.Cache(num_bands=20, hasher=hasher)\n",
    "neardup_ids = []\n",
    "\n",
    "# hash every text\n",
    "for idx, text in enumerate(tqdm(df['text'])):\n",
    "    lsh_cache.add_fingerprint(hasher.fingerprint(text), idx)\n",
    "\n",
    "# find bins of duplicates\n",
    "for cache_bin in lsh_cache.bins:\n",
    "    for bucket_id in cache_bin:\n",
    "        if len(cache_bin[bucket_id]) > 1:\n",
    "            # add ids of neardup texts\n",
    "            neardup_ids.append(cache_bin[bucket_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sets = sorted(neardup_ids, key=lambda x: min(x))\n",
    "\n",
    "merged_sets = []\n",
    "current_merged_set = sorted_sets[0]\n",
    "\n",
    "for s in sorted_sets[1:]:\n",
    "    # if has overlapping element\n",
    "    if any(x in current_merged_set for x in s):\n",
    "        current_merged_set.update(s)  # merge overlapping sets\n",
    "    else:\n",
    "        merged_sets.append(current_merged_set)\n",
    "        current_merged_set = s  # start a new merged set\n",
    "\n",
    "# add last set\n",
    "merged_sets.append(current_merged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ids = set()\n",
    "\n",
    "# for each \"cluster\", only keep first and drop the rest\n",
    "for cluster in merged_sets:\n",
    "    drop_ids |= set(list(cluster)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda row: row.name not in drop_ids, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 654056, 1: 845})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mask_reddit_comments(comment):\n",
    "    # Mask usernames with <username>\n",
    "    comment = re.sub(r'/u/[\\w]+', '<username>', comment)\n",
    "    # Mask hashtags with <hashtag>\n",
    "    comment = re.sub(r'#[\\w]+', '<hashtag>', comment)\n",
    "    # Mask email addresses with <email>\n",
    "    comment = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w{2,4}\\b', '<email>', comment)\n",
    "    # Mask links/URLs with <link> (handling various URL formats)\n",
    "    comment = re.sub(r'https?://\\S+|www\\.\\S+', '<link>', comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58611/3086847846.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"text\"] = df[\"text\"].apply(mask_reddit_comments)\n"
     ]
    }
   ],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(mask_reddit_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/root/reddit_indonesia_sarcastic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp = df[df['label'] == 1]\n",
    "# tmp[['text', 'label']].head(n=50).to_html('/root/temp.html', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
