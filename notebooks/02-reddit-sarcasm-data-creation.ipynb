{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/mnt/block-volume/root/reddit_comments_subreddit_indonesia_RC_2020-01-2023-09.json\") as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4735944"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"author\", \"created_utc\", \"score\", \"permalink\", \"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data = []\n",
    "\n",
    "for datum in raw_data:\n",
    "    # split multi-sentence\n",
    "    body = datum[\"body\"]\n",
    "    sentences = body.split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        obj = {k: datum[k] for k in columns}\n",
    "        text = sentence.strip()\n",
    "        if len(text) > 0:\n",
    "            obj['body'] = text\n",
    "            sentences_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/7066715 [00:00<?, ?it/s]Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 7066715/7066715 [02:10<00:00, 54162.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from ftlangdetect import detect\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# indonesian, javanese, minangkabau, malaysian, sundanese\n",
    "valid_langs = [\"id\", \"jv\", \"min\", \"ms\", \"su\"]\n",
    "indonesia_data = []\n",
    "\n",
    "for datum in tqdm(sentences_data):\n",
    "    lang = detect(datum[\"body\"])['lang']\n",
    "    if lang in valid_langs:\n",
    "        datum['lang_fastText'] = lang\n",
    "        indonesia_data.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789839"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indonesia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_sarcasm_tag(text: str) -> bool:\n",
    "    # ends with either one of these sarcasm tags\n",
    "    pattern = r'(?<!\\S)(?:/s|//s|/sarcasm|//sarcasm|\\\\s|\\\\\\\\s|\\\\sarcasm|\\\\\\\\sarcasm)$'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "\n",
    "for datum in indonesia_data:\n",
    "    body = datum[\"body\"].replace('\\x00', '')\n",
    "    datum[\"body\"] = body\n",
    "    tags = has_sarcasm_tag(body)\n",
    "    if tags:\n",
    "        # make `text` column clean; remove sarcasm tags\n",
    "        for tag in tags:\n",
    "            body = body.replace(tag, \"\")\n",
    "\n",
    "    datum[\"label\"] = 1 if tags else 0\n",
    "    datum[\"text\"] = body.strip()\n",
    "    cleaned_data.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(cleaned_data)\n",
    "df['created_utc'] = df['created_utc'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 3784860, 1: 4979})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1904/3789839 [00:00<09:32, 6614.04it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3789839/3789839 [11:08<00:00, 5671.61it/s] \n"
     ]
    }
   ],
   "source": [
    "from lsh import minhash, cache\n",
    "\n",
    "# use minHash LSH algorithm to find near duplicates\n",
    "hasher = minhash.MinHasher(seeds=100, char_ngram=4, hashbytes=8, random_state=42)\n",
    "lsh_cache = cache.Cache(num_bands=20, hasher=hasher)\n",
    "neardup_ids = []\n",
    "\n",
    "# hash every text\n",
    "for idx, text in enumerate(tqdm(df['text'])):\n",
    "    lsh_cache.add_fingerprint(hasher.fingerprint(text), idx)\n",
    "\n",
    "# find bins of duplicates\n",
    "for cache_bin in lsh_cache.bins:\n",
    "    for bucket_id in cache_bin:\n",
    "        if len(cache_bin[bucket_id]) > 1:\n",
    "            # add ids of neardup texts\n",
    "            neardup_ids.append(cache_bin[bucket_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sets = sorted(neardup_ids, key=lambda x: min(x))\n",
    "\n",
    "merged_sets = []\n",
    "current_merged_set = sorted_sets[0]\n",
    "\n",
    "for s in sorted_sets[1:]:\n",
    "    # if has overlapping element\n",
    "    if any(x in current_merged_set for x in s):\n",
    "        current_merged_set.update(s)  # merge overlapping sets\n",
    "    else:\n",
    "        merged_sets.append(current_merged_set)\n",
    "        current_merged_set = s  # start a new merged set\n",
    "\n",
    "# add last set\n",
    "merged_sets.append(current_merged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ids = set()\n",
    "\n",
    "# for each \"cluster\", only keep first and drop the rest\n",
    "for cluster in merged_sets:\n",
    "    drop_ids |= set(list(cluster)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.apply(lambda row: row.name not in drop_ids, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2616335, 1: 3529})"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mask_reddit_comments(comment):\n",
    "    # Mask usernames with <username>\n",
    "    comment = re.sub(r'/u/[\\w]+', '<username>', comment)\n",
    "    # Mask hashtags with <hashtag>\n",
    "    comment = re.sub(r'#[\\w]+', '<hashtag>', comment)\n",
    "    # Mask email addresses with <email>\n",
    "    comment = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w{2,4}\\b', '<email>', comment)\n",
    "    # Mask links/URLs with <link> (handling various URL formats)\n",
    "    comment = re.sub(r'https?://\\S+|www\\.\\S+', '<link>', comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df[\"text\"].apply(mask_reddit_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/mnt/block-volume/root/reddit_indonesia_sarcastic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
