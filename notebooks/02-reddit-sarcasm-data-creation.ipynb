{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/mnt/block-volume/root/reddit_comments_subreddit_indonesia_RC_2020-01-2023-09.json\") as f:\n",
    "    raw_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4735944"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"author\", \"created_utc\", \"score\", \"permalink\", \"subreddit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_data = []\n",
    "\n",
    "for datum in raw_data:\n",
    "    # split multi-sentence\n",
    "    body = datum[\"body\"]\n",
    "    sentences = body.split(\"\\n\")\n",
    "    for sentence in sentences:\n",
    "        obj = {k: datum[k] for k in columns}\n",
    "        text = sentence.strip()\n",
    "        if len(text) > 0:\n",
    "            obj['body'] = text\n",
    "            sentences_data.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/7066715 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "100%|██████████| 7066715/7066715 [02:07<00:00, 55554.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from ftlangdetect import detect\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# indonesian, javanese, minangkabau, malaysian, sundanese\n",
    "valid_langs = [\"id\", \"jv\", \"min\", \"ms\", \"su\"]\n",
    "indonesia_data = []\n",
    "\n",
    "for datum in tqdm(sentences_data):\n",
    "    lang = detect(datum[\"body\"])['lang']\n",
    "    if lang in valid_langs:\n",
    "        datum['lang_fastText'] = lang\n",
    "        indonesia_data.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3789839"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indonesia_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def has_sarcasm_tag(text: str) -> bool:\n",
    "    # ends with either one of these sarcasm tags\n",
    "    pattern = r'(?<!\\S)(?:/s|//s|/sarcasm|//sarcasm|\\\\s|\\\\\\\\s|\\\\sarcasm|\\\\\\\\sarcasm)$'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches if matches else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "\n",
    "for datum in indonesia_data:\n",
    "    body = datum[\"body\"].replace('\\x00', '')\n",
    "    datum[\"body\"] = body\n",
    "    tags = has_sarcasm_tag(body)\n",
    "    if tags:\n",
    "        # make `text` column clean; remove sarcasm tags\n",
    "        for tag in tags:\n",
    "            body = body.replace(tag, \"\")\n",
    "\n",
    "    datum[\"label\"] = 1 if tags else 0\n",
    "    datum[\"text\"] = body.strip()\n",
    "    cleaned_data.append(datum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3789839/3789839 [10:58<00:00, 5755.09it/s] \n"
     ]
    }
   ],
   "source": [
    "from lsh import minhash, cache\n",
    "\n",
    "# use minHash LSH algorithm to find near duplicates\n",
    "hasher = minhash.MinHasher(seeds=100, char_ngram=4, hashbytes=8, random_state=42)\n",
    "lsh_cache = cache.Cache(num_bands=20, hasher=hasher)\n",
    "neardup_ids = []\n",
    "\n",
    "# hash every text\n",
    "texts = [d['text'] for d in cleaned_data]\n",
    "for idx, text in enumerate(tqdm(texts)):\n",
    "    lsh_cache.add_fingerprint(hasher.fingerprint(text), idx)\n",
    "\n",
    "# find bins of duplicates\n",
    "for cache_bin in lsh_cache.bins:\n",
    "    for bucket_id in cache_bin:\n",
    "        if len(cache_bin[bucket_id]) > 1:\n",
    "            # add ids of neardup texts\n",
    "            neardup_ids.append(cache_bin[bucket_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sets = sorted(neardup_ids, key=lambda x: min(x))\n",
    "\n",
    "merged_sets = []\n",
    "current_merged_set = sorted_sets[0]\n",
    "\n",
    "for s in sorted_sets[1:]:\n",
    "    # if has overlapping element\n",
    "    if any(x in current_merged_set for x in s):\n",
    "        current_merged_set.update(s)  # merge overlapping sets\n",
    "    else:\n",
    "        merged_sets.append(current_merged_set)\n",
    "        current_merged_set = s  # start a new merged set\n",
    "\n",
    "# add last set\n",
    "merged_sets.append(current_merged_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_ids = set()\n",
    "\n",
    "# for each \"cluster\", only keep first and drop the rest\n",
    "for cluster in merged_sets:\n",
    "    drop_ids |= set(list(cluster)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_data = [d for i, d in enumerate(cleaned_data) if i not in drop_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 2616335, 1: 3529})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([d['label'] for d in deduplicated_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mask_reddit_comments(comment):\n",
    "    # Mask usernames with <username>\n",
    "    comment = re.sub(r'/u/[\\w]+', '<username>', comment)\n",
    "    # Mask hashtags with <hashtag>\n",
    "    comment = re.sub(r'#[\\w]+', '<hashtag>', comment)\n",
    "    # Mask email addresses with <email>\n",
    "    comment = re.sub(r'\\b[\\w.-]+?@\\w+?\\.\\w{2,4}\\b', '<email>', comment)\n",
    "    # Mask links/URLs with <link> (handling various URL formats)\n",
    "    comment = re.sub(r'https?://\\S+|www\\.\\S+', '<link>', comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datum in deduplicated_data:\n",
    "    datum['text'] = mask_reddit_comments(datum['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/block-volume/root/reddit_indonesia_sarcastic/raw_data/reddit_indonesia_sarcastic.json\", \"w\") as f:\n",
    "    json.dump(deduplicated_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarcastic = [d for d in deduplicated_data if d['label'] == 1]\n",
    "non_sarcastic = [d for d in deduplicated_data if d['label'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(41)\n",
    "sampled_non_sarcastic = random.sample(non_sarcastic, k=len(sarcastic) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = sampled_non_sarcastic + sarcastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_val_data = train_test_split(balanced_data, train_size=0.7, random_state=41, stratify=[d['label'] for d in balanced_data])\n",
    "val_data, test_data = train_test_split(test_val_data, test_size=(2/3), random_state=41, stratify=[d['label'] for d in test_val_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9881, 2824, 1411)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data), len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/mnt/block-volume/root/reddit_indonesia_sarcastic/data/train.json\", \"w\") as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(\"/mnt/block-volume/root/reddit_indonesia_sarcastic/data/test.json\", \"w\") as f:\n",
    "    json.dump(test_data, f)\n",
    "\n",
    "with open(\"/mnt/block-volume/root/reddit_indonesia_sarcastic/data/validation.json\", \"w\") as f:\n",
    "    json.dump(val_data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
